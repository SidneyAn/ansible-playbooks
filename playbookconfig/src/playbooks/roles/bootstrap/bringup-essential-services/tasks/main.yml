---
#
# Copyright (c) 2019 Wind River Systems, Inc.
#
# SPDX-License-Identifier: Apache-2.0
#
# ROLE DESCRIPTION:
#   This role is to bring up Kubernetes and essential flock services required
#   for initial controller unlock.
#

- block:
  - name: Add loopback interface
    # Use shell instead of command module as source is an internal shell command
    shell: "{{ item }}"
    register: add_addresses
    failed_when: false
    with_items:
      - source /etc/platform/openrc; system host-if-add controller-0 lo virtual none lo -c platform -m 1500
      - source /etc/platform/openrc; system interface-network-assign controller-0 lo mgmt
      - source /etc/platform/openrc; system interface-network-assign controller-0 lo cluster-host
      - ip addr add {{ cluster_virtual }}  brd {{ cluster_broadcast }} dev lo scope host label lo:5
      - ip addr add {{ mgmt_virtual }} brd {{ management_broadcast }} dev lo scope host label lo:1
      - ip addr add {{ pxe_virtual }} dev lo scope host
      - ip addr add {{ cluster_floating_virtual }} dev lo scope host
      - ip addr add {{ mgmt_nfs_1_virtual }} dev lo scope host
      - ip addr add {{ mgmt_nfs_2_virtual }} dev lo scope host

  - name: Fail if adding interface addresses failed for reason other than it has been done before
    fail:
      msg: "{{ item.item }} failed for reason: {{ item.stderr }}."
    when: item.rc != 0 and not incomplete_bootstrap
    with_items: "{{ add_addresses.results }}"

  - name: Remove previous management floating address if management network config has changed
    command: ip addr delete {{ prev_mgmt_floating_virtual }} dev lo scope host
    when: last_config_file_exists and reconfigure_endpoints and
          (mgmt_floating_virtual != prev_mgmt_floating_virtual)

  - name: Refresh local DNS (i.e. /etc/hosts)
    import_tasks: refresh_local_dns.yml

  - name: Set up default route to the oam gateway
    include_tasks: setup_default_route.yml

  - name: Copy the central registry certificate
    include_tasks: copy_central_registry_cert.yml
    when: distributed_cloud_role == 'subcloud'

  - name: Load images from archives if configured
    include_tasks: load_images_from_archive.yml
    when: images_archive_exists

  - name: Bring up local docker registry
    import_tasks: bringup_local_registry.yml

  - name: Push images to local docker registry
    import_role:
      name: common/push-docker-images

  - name: Bring up Kubernetes master
    import_tasks: bringup_kubemaster.yml

  - name: Bring up Helm
    import_tasks: bringup_helm.yml

  - name: Bring up essential flock services
    import_tasks: bringup_flock_services.yml

  - name: Set dnsmasq.leases flag for unlock
    file:
      path: "{{ config_permdir }}/dnsmasq.leases"
      state: touch

  - name: Update resolv.conf file for unlock
    lineinfile:
      path: /etc/resolv.conf
      line: "nameserver {{ controller_floating_address }}"
      insertbefore: BOF

  - name: Check controller-0 is in online state
    shell: source /etc/platform/openrc; system host-show controller-0 --column availability --format value
    register: check_online
    retries: 10
    until: check_online.stdout == "online"

  - name: Wait for {{ pods_wait_time }} seconds to ensure kube-system pods are all started
    wait_for:
      timeout: "{{ pods_wait_time }}"

  - name: Set async parameters
    set_fact:
      async_timeout: 30
      async_retries: 10

  - name: Set Kubernetes components list
    set_fact:
      kube_component_list:
        - k8s-app=calico-node
        - k8s-app=calico-kube-controllers
        - k8s-app=kube-proxy
        - app=multus
        - app=sriov-cni
        - app=helm
        - component=kube-apiserver
        - component=kube-controller-manager
        - component=kube-scheduler

  - block:
    - name: Update Kubernetes components list
      set_fact:
        # We skip the calico-node pod on AIO-DX and STANDARD setups
        # because the pods running on a different host than controller-0 will
        # be unreachable at this moment and the calico-node pods
        # will try to connect to them and fail forever
        kube_component_list: "{{ kube_component_list | reject('search', 'calico-node') | list }}"

    - name: Override async parameters
      set_fact:
        async_timeout: 120
        async_retries: 20
      when: system_type == 'Standard'

    when: mode == 'restore'

  - name: Start parallel tasks to wait for Kubernetes component, Networking and Tiller pods to reach ready state
    command: >-
      kubectl --kubeconfig=/etc/kubernetes/admin.conf wait  --namespace=kube-system
      --for=condition=Ready pods --selector {{ item }} --timeout=30s
    async: "{{ async_timeout }}"
    poll: 0
    with_items: "{{ kube_component_list }}"
    register: wait_for_pods

  - name: Get wait tasks results
    async_status:
      jid: "{{ item.ansible_job_id }}"
    register: wait_job_result
    until: wait_job_result.finished
    # Set the retry to 10 times (60 seconds) but the async jobs above will
    # complete (success or failure) within 30 seconds
    retries: "{{ async_retries }}"
    # At B&R, after the restore phase, this will fail on duplex or standard systems because
    # some of the resources that we are waiting for are replicasets and daemonsets
    # and some pods will be launched on a different host than controller-0.
    # Since only the controller-0 is online at this step, the rest of the pods that
    # are on a different host will fail to start, so we only need to check that
    # at least 1 pod from every deployment is up and running. If there are none active
    # from a particular deployment it will be caught in the next task.
    failed_when: false
    with_items: "{{ wait_for_pods.results }}"

  - name: Fail if any of the Kubernetes component, Networking and Tiller pods is not ready by this time
    fail:
      msg: "Pod {{ item._ansible_item_label._ansible_item_label }} is still not ready."
    when: item.stdout is not search(" condition met")
    with_items: "{{ wait_job_result.results }}"

  # Have to check for kube-dns pods separately as at most only one is
  # running at this point so checking for "Ready" condition at kube-dns
  # app level won't work
  - name: Fail if no kube-dns pod is running
    shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf get pods --namespace=kube-system | grep coredns | grep Running
    register: dns_pod_result
    failed_when: dns_pod_result.rc != 0

  when: (not replayed) or (restart_services)
